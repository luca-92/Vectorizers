{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HashingVectorizer\n",
    "CountVectorization and TfidfVectorization scheme are simple but the fact that it holds an in- memory mapping from the string tokens to the integer feature indices (the vocabulary_ attribute) causes several problems when dealing with large datasets:\n",
    "<ol>\n",
    "<li> The larger the corpus, the larger the vocabulary will grow and hence the memory use too, \n",
    "<li> fitting requires the allocation of intermediate data structures of size proportional to that of the original dataset.\n",
    "<li> Building the word-mapping requires a full pass over the dataset hence it is not possible to fit text classifiers in a strictly online manner.\n",
    "<li> Pickling and un-pickling vectorizers with a large vocabulary_ can be very slow (typically much slower than pickling / un-pickling flat data structures such as a NumPy array of the same size),\n",
    "<li> it is not easily possible to split the vectorization work into concurrent sub tasks as the vocabulary_ attribute would have to be a shared state with a fine grained synchronization barrier: the mapping from token string to feature index is dependent on ordering of the first occurrence of each token hence would have to be shared, potentially harming the concurrent workers’ performance to the point of making them slower than the sequential variant.\n",
    "</ol>\n",
    "HashingVectorizer is stateless, meaning that you don’t have to call <b>fit</b> on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.31622777,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.31622777, -0.63245553,  0.63245553],\n       [ 0.        , -0.31622777,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.31622777, -0.63245553,  0.63245553]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "text = [\"the cat is on the table\",\n",
    "        \"the dog is on the table\"]\n",
    "vectorizer = HashingVectorizer(n_features=10)\n",
    "vector = vectorizer.transform(text)\n",
    "vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of values different from 0 are less than the total number of word in the document. This is due to the collision of the hash function that is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
